{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Smartcab Report\n",
    "<p> Alex Leu </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?**\n",
    "\n",
    "The agent has the motion of either going none, forward, left and right. If there is an oncoming car, or the traffic light is red, it would prevent the agent from moving forward. The agent eventually makes it to the target locaiton when the dealine criteria is turned to \"False.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justify why you picked these set of states, and how they model the agent and its environment.**\n",
    "\n",
    "The state shows whether there is a green/red lights, the next action of where the agent is going to, and the deadline. It can either go \"none\", \"forward\", \"left\", \"right\".\n",
    "\n",
    "The directions were picked as they were the only valid actions the agents can take. The states were shown to be what could get affect the motion of the agent, referenced from the dummy agent.\n",
    "\n",
    "By setting up as the deadline limit, it provides limited number of tries the agent can do to get to the destination. Addtionally, we do not want the number of deadline variable to blow up state space into a size that can't be explored by the agent in 100 trials. If the agent could never get to the destination, the program would have been running non-stop with out the deadline.\n",
    "\n",
    "Within the Q-modeling, the state picks up whether the traffic light is green/red, 'oncoming', 'left, and waypoint. I did not take in 'right', as knowing the status of the 'left' will indicate whether there is a right. Traffic light provides whether the car can move or not, 'oncoming' shows whether there is a car that is blocking the agent's way, 'left' shows how the car is making it's next poing, and the waypoint shows where the destination is.\n",
    "\n",
    "Another reason to exclude the traffic to the right, as the car in the State has a right-hand rule, where the car can turn right even if the the traffic light is red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What changes do you notice in the agent’s behavior?**\n",
    "\n",
    "It stays stationary a lot at first. After it picks up, and gets close to the target, it often turns in a circle.\n",
    "We can see the agent is learning, and trying to reach to the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?**\n",
    "\n",
    "Here are the list of changes regarding to the constants for alpha, gamma, and epison. We went through the trial and erorr to find out the best tuning for the algorithmn. By changing/tuning over the values, we found the the tuning is at the most optimal at.\n",
    "\n",
    "Here's the chart for the tuning for 3 trials:\n",
    "\n",
    "alpha|gamma|episilon|avg_deadline|success_rate|cumulative rewards\n",
    "---|---|---|---|---|\n",
    "1|0.25|0.25|22.3|100%|1.5\n",
    "1|0.25|0.5|4.3|33%|2\n",
    "1|0.25|0.75|0|0%|2.5\n",
    "1|0.25|1|0|0%|-2\n",
    "1|0.5|0.25|3.6|33%|2\n",
    "1|0.5|0.5|0|0%|-2\n",
    "0.75|0.5|0.5|-0|0%|-0.5\n",
    "0.75|0.75|0.5|8\n",
    "0.75|0.75|0.75|-3\n",
    "0.5|0.75|0.75|-6\n",
    "0.5|1|0.75|-112\n",
    "0.5|1|1|-82\n",
    "\n",
    "The trial was run 2 times for each parameter, and takes the average of the deadline to get the number. I am currently basing on how well the car is performing by letting the car run as long as possible, and see how long does it take for it to get to the destination.\n",
    "\n",
    "Ideally, the optimal policy would have the agent be aware of the location of the destination consistently. An ideal policy would also provide a detection of the oncoming cars in a bigger radius then the current state, where the agent only found the other cars through intersections.\n",
    "\n",
    "Another optimal policy would be stricter at the law breaking. If the car is breaking the law by passing the red lights, add in harsher rewards reduction to prevent the car from breaking the law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
