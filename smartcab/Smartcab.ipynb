{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Smartcab Report\n",
    "<p> Alex Leu </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In your report, mention what you see in the agent’s behavior. Does it eventually make it to the target location?**\n",
    "\n",
    "The agent has the motion of either going none, forward, left and right. If there is an oncoming car, or the traffic light is red, it would prevent the agent from moving forward. The agent eventually makes it to the target locaiton when the dealine criteria is turned to \"False.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justify why you picked these set of states, and how they model the agent and its environment.**\n",
    "\n",
    "The state shows whether there is a green/red lights, the next action of where the agent is going to, and the deadline. It can either go \"none\", \"forward\", \"left\", \"right\".\n",
    "\n",
    "The directions were picked as they were the only valid actions the agents can take. The states were shown to be what could get affect the motion of the agent, referenced from the dummy agent.\n",
    "\n",
    "By setting up as the deadline limit, it provides limited number of tries the agent can do to get to the destination. Addtionally, we do not want the number of deadline variable to blow up state space into a size that can't be explored by the agent in 100 trials. If the agent could never get to the destination, the program would have been running non-stop with out the deadline.\n",
    "\n",
    "Within the Q-modeling, the state picks up whether the traffic light is green/red, 'oncoming', 'left, and waypoint. I did not take in 'right', as knowing the status of the 'left' will indicate whether there is a right. Traffic light provides whether the car can move or not, 'oncoming' shows whether there is a car that is blocking the agent's way, 'left' shows how the car is making it's next poing, and the waypoint shows where the destination is.\n",
    "\n",
    "Another reason to exclude the traffic to the right, as the car in the State has a right-hand rule, where the car can turn right even if the the traffic light is red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What changes do you notice in the agent’s behavior?**\n",
    "\n",
    "It stays stationary a lot at first. After it picks up, and gets close to the target, it often turns in a circle.\n",
    "We can see the agent is learning, and trying to reach to the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report what changes you made to your basic implementation of Q-Learning to achieve the final version of the agent. How well does it perform?\n",
    "Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties?**\n",
    "\n",
    "Here are the list of changes regarding to the constants for alpha, gamma, and epison. We went through the trial and erorr to find out the best tuning for the algorithmn. By changing/tuning over the values, we found the the tuning is at the most optimal at.\n",
    "\n",
    "Here's the chart for the tuning for 3 trials:\n",
    "\n",
    "alpha|gamma|episilon|avg_deadline|success_rate|cumulative rewards\n",
    "---|---|---|---|---|\n",
    "1|0.25|0.25|22.3|100%|1.5\n",
    "1|0.25|0.5|4.3|33%|2\n",
    "1|0.25|0.75|0|0%|-1.5\n",
    "1|0.25|1|0|0%|-2\n",
    "1|0.5|0.25|3.6|33%|2\n",
    "1|0.5|0.5|0|0%|-2\n",
    "0.75|0.5|0.5|-0|0%|-0.5\n",
    "0.75|0.75|0.5|0|0%|1\n",
    "0.75|0.75|0.75|9.6|33%|1.5\n",
    "\n",
    "\n",
    "The trial was run 3 times for each parameter, and takes the average of the deadline to get the number, successful rate out of 3 trials, and cumulative rewards from the successful trips. It seems like the agent does well when the alpha == 1, and the gamma and episilon is on the lower side.\n",
    "\n",
    "\n",
    "Within my model, there are times when the agent just goes in circle, and it unable to find the destination. Also at the beginning the car would not move even though it is safe for the agent to travel. I also realized when the destination is closed by on the right side of the agent, it takes awhile for the agent to recognize the destination, and then follow through to the right direction.\n",
    "\n",
    "The ideal optimal policy would be able to have the location to be spotted by the agent, and build the next step based on where the locaiton is at. Additionally, the agent does not always obey the traffic law, and would take right turn ont eh red traffic light. To prevent the learning agent from breaking the traffic laws would be adding more penalty when the agent is not breaking the law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
